---
title: "Project 2: IMDB Ratings using Random Forest"
author: "Team: I Love Lucy"
date: "2 Nov 2024"
output:
  html_document:
    toc: true
    number_sections: true
    self_contained: true
python: 
  jupyter: data622
execute:
#  echo: false
  warning: false
  message: false
  freeze: auto
---
<style>
.quarto-title-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    flex-wrap: wrap;
}

.quarto-title-meta-heading {
    font-weight: bold;
}

.quarto-title-meta-contents {
    margin-right: 20px;
}

body {
    width: 900px; /* Lock the body width to 900px */
    font-family: Arial, sans-serif;
    margin: 0 auto; /* Center the body */
    background-color: white; /* Set background to white */
}

/* Flexbox container for title and author */
.header-container {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 20px; /* Add space below the header */
}

.header-container h1 {
    margin: 0;
    font-size: 2.5em;
}

.header-container .meta-info {
    text-align: right; /* Align the meta information (author, date) to the right */
    font-size: 1.2em;
    margin: 0;
}

h2, h3, h4, h5, h6 {
    font-family: Arial, sans-serif;
    margin: 0 0 10px 0; /* Reduce the bottom margin for more compact headers */
    padding: 0; /* Remove padding */
    line-height: 1.2; /* Control the line spacing */
}

/* Adjust table and image styles */
table {
    width: 100%; /* Make table full width within the 900px body */
    border-collapse: collapse;
    max-width: 100%;
    margin-left: auto;  /* Center the table */
    margin-right: auto; /* Center the table */
    overflow-x: auto; /* Allow horizontal scrolling if the table is too wide */
    display: block;
}

table, th, td {
    border: 1px solid lightgray;
    padding: 8px;
    text-align: left;
}

th {
    background-color: #f2f2f2;
}

/* Custom figure sizing */
.figure {
    width: 100%; /* Ensure figures take full width within the 900px body */
    margin-left: auto;  /* Center figure */
    margin-right: auto; /* Center figure */
}

img {
    max-width: 100%;  /* Ensure images take full width within the 900px body */
    height: auto;
    display: block;
    margin-left: auto;  /* Center image */
    margin-right: auto; /* Center image */
}
</style>
<!-- build with:  ./build.sh -p 2 -h -->
<p style="text-align: center;">
  [github](https://github.com/tonythor/data622) &nbsp; | &nbsp; [web presentation](https://rpubs.com/tonythor/data622-project2)
</p>


## Project Overview
In this project, we set out to use Random Forest on the [IMDB non-commercial dataset](https://developer.imdb.com/non-commercial-datasets/)to predict movie ratings. While the dataset lacks financial performance metrics like box office revenue or streaming views, it provides key features such as ratings, genres, and cast information, which became the foundation of our analysis.

This project provided an ideal test case for exploring decision tree limitations, as movie ratings involve both categorical (genres) and numerical (runtime) features, similar to many real world applications.

One of the biggest challenges we encountered was the sheer number of unique actors. With millions of actors in the dataset, one-hot encoding was not practical. To address this, we developed a system to measure actor experience through an average-based metric and a Likert-scale score for each movie’s cast, giving us a way to quantify experience without overwhelming the model.

Despite the challenges, our efforts paid off. From the initial runs, we achieved promising predictive results, showing that thoughtful feature engineering can unlock valuable insights even in large, complex datasets.

## Data Preparation

Our workflow to prepare the data consisted of three stages:

1. **Download the Raw Datasets**  
   We downloaded multiple IMDB datasets, such as **ratings**, **basics**, and **principals**, and saved them locally for processing.

2. **Merge Datasets by Movie Title**  
   Using each movie's unique identifier (`tconst`), we merged datasets to create a **single, consolidated DataFrame**, which we persisted for efficiency.

3. **Add Calculated Columns**  
   After merging, we added several **calculated columns** (detailed below) to enrich the data with features like **actor experience** and **genre dummy variables** for better predictive power.


## Calculated Columns Overview

Below is a summary of the calculated columns added and their purpose:

- **`num_actors`**:  
  Total number of actors in each movie. Helps capture the cast size.

- **`actor_names`**:  
  A string of all actor names for each movie, separated by commas. Useful for analyzing trends or patterns based on cast members.

- **`experienced_actor_count`**:  
  Counts the number of experienced actors (those with more than 10 prior roles) in a movie. Measures the potential impact of experience on quality or reception.

- **`experienced_actors_likert`**:  
  A Likert-scale score (1–5) based on the average experience of the cast. Quantifies cast experience for easier analysis of its effect on ratings.

- **`rating_bin`**:  
  Binned version of the average rating (e.g., 1–10). Simplifies predictions by grouping continuous ratings into categories, which aligns with classification models like Random Forest.

- **Genre Dummy Variables**:  
  Each genre is expanded into a binary (0/1) column. Provides genre-specific features to analyze how genres influence ratings.


## Data Overview
```{python load_01, message=false, warning=false, echo=FALSE}
data_dir = "622data_nogit/imdb"
from lussi.imdb import *
from lussi.glimpse import *
df = load_imdb(data_dir)
glimpse(df)
```
## Model Comparisons

### Decision Tree Models
First, we'll establish baseline performance using decision trees of varying complexity.

#### Basic Decision Tree Model
```{python basic_tree_02}
# Train basic decision tree without vote counts
df_cleaned = df.drop(columns=['tconst', 'primaryTitle', 'actor_names', 'numVotes'])
basic_tree, basic_metrics, basic_preds = train_and_evaluate_basic_tree(
    df_cleaned,
    cache_path='basic_tree_no_votes.joblib',
    rerun=False,
    show_output=False
)
generate_summary_report(*basic_preds, "Basic Decision Tree")
```

#### Complex Decision Tree Model
```{python complex_tree_03}
# Train complex decision tree without vote counts
complex_tree, complex_metrics, complex_preds = train_and_evaluate_complex_tree(
    df_cleaned,
    cache_path='complex_tree_no_votes.joblib',
    rerun=False,
    show_output=False
)
generate_summary_report(*complex_preds, "Complex Decision Tree")
```

### Random Forest Models
We'll examine Random Forest performance under two scenarios: including vote counts (which could introduce bias) and excluding them (more realistic for new movies).

#### With Vote Counts
While using vote counts doesn't reflect real-world prediction scenarios (since we wouldn't have this data for new movies), we include it to test the model's potential performance with this information.
``` {python rf_including_vote_counts_04}
# Train Random Forest including vote counts
df_with_votes = df.drop(columns=['tconst', 'primaryTitle', 'actor_names'])
rf_votes, importance_votes, metrics_votes, predictions_votes = train_and_evaluate_rf(
    df_with_votes,
    cache_path='rf_with_votes.joblib',
    rerun=False,
    show_output=False
)
generate_summary_report(*predictions_votes, "Random Forest (with votes)")
# display(plot_importance(importance_votes))

```

#### Without Vote Counts
This scenario better reflects real-world use where we predict ratings for new movies without vote information.

```{python rf_excluding_vote_counts_05}
# Train Random Forest excluding vote counts
rf_no_votes, importance_no_votes, metrics_no_votes, predictions_no_votes = train_and_evaluate_rf(
    df_cleaned,
    cache_path='rf_no_votes.joblib',
    rerun=False,
    show_output=False
)
generate_summary_report(*predictions_no_votes, "Random Forest (without votes)")
# display(plot_importance(importance_no_votes))
```

## Comparing Our Approaches

We tackled the challenge of predicting movie ratings using decision trees, learning from common problems highlighted in "The GOOD, The BAD & The UGLY of Using Decision Trees." Here's how we built and improved our approach:

1. **Simple Decision Tree**
- Used just basic features like movie length and genres
- Often made big mistakes when movies didn't fit typical patterns
- Showed us why single decision trees can be too rigid

2. **Complex Decision Tree**
- Added more features like actor experience scores
- Got slightly better results but started to look messy
- Demonstrated the "memorization" problem mentioned in the blog

3. **Random Forest Solution**
- Combined many trees to avoid single tree problems
- Performed more consistently across different types of movies
- Solved many issues the blog warned about

The blog mentioned how decision trees can "blow up" with too many options - we saw this firsthand. Our simple tree was too basic, our complex tree started memorizing data, but the Random Forest found a good balance.


## Learning From Past Decision Tree Problems

The blog warned about several real world problems with decision trees. Here's how we dealt with each one:

**Problem 1: Trees That Memorize Instead of Learn**
- Blog Example: Customer service chatbots that only work for exact matches
- Our Solution: Used Random Forest to average out individual tree mistakes
- Result: More reliable predictions even for unusual movies

**Problem 2: Too Many Options**
- Blog Example: Product recommendation trees that get overwhelmed with too many options (like having thousands of individual products)
- Our Solution: Created simpler categories by converting thousands of actors into simple 1-5 experience scores
- Result: Made predictions possible without getting lost in endless detail

**Problem 3: Hard to Maintain**
- Blog Example: Support systems that break when adding new options
- Our Solution: Created features that work automatically for new movies
- Result: System that can handle new actors and movies

## Conclusion

Our work with decision trees taught us several important lessons:
- Simple trees can work, but need careful feature design
- The problems mentioned in the blog are real - we hit most of them
- Random Forests help, but good data preparation matters more
- Getting 80% accuracy within one rating point shows this approach works

While our approach successfully tackles the main problems discussed in the blog, we see several ways to make it even better:
- Adding automated updates for new movies
- Testing more thoroughly with different types of movies
- Using more specific actor traits beyond just experience

Most importantly, we learned that success with decision trees isn't just about the model - it's about understanding their limitations and designing solutions that work around them. Our approach might not be perfect, but it demonstrates how thoughtful feature engineering and Random Forests can overcome common decision tree problems while delivering practical value.
