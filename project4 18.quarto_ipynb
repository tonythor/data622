{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Project 4: MNIST using KNN and Neural Networks\"\n",
        "author: \"Team: I Love Lucy\"\n",
        "date: \"8 Dec 2024\"\n",
        "output:\n",
        "  html_document:\n",
        "    toc: true\n",
        "    number_sections: true\n",
        "    self_contained: true\n",
        "python: \n",
        "  jupyter: data622\n",
        "execute:\n",
        "#  echo: false\n",
        "  warning: false\n",
        "  message: false\n",
        "  freeze: auto\n",
        "---\n",
        "\n",
        "<style>\n",
        ".quarto-title-meta {\n",
        "    display: flex;\n",
        "    justify-content: space-between;\n",
        "    align-items: center;\n",
        "    flex-wrap: wrap;\n",
        "}\n",
        "\n",
        ".quarto-title-meta-heading {\n",
        "    font-weight: bold;\n",
        "}\n",
        "\n",
        ".quarto-title-meta-contents {\n",
        "    margin-right: 20px;\n",
        "}\n",
        "\n",
        "body {\n",
        "    width: 900px; /* Lock the body width to 900px */\n",
        "    font-family: Arial, sans-serif;\n",
        "    margin: 0 auto; /* Center the body */\n",
        "    background-color: white; /* Set background to white */\n",
        "}\n",
        "\n",
        "/* Flexbox container for title and author */\n",
        ".header-container {\n",
        "    display: flex;\n",
        "    justify-content: space-between;\n",
        "    align-items: center;\n",
        "    margin-bottom: 20px; /* Add space below the header */\n",
        "}\n",
        "\n",
        ".header-container h1 {\n",
        "    margin: 0;\n",
        "    font-size: 2.5em;\n",
        "}\n",
        "\n",
        ".header-container .meta-info {\n",
        "    text-align: right; /* Align the meta information (author, date) to the right */\n",
        "    font-size: 1.2em;\n",
        "    margin: 0;\n",
        "}\n",
        "\n",
        "h2, h3, h4, h5, h6 {\n",
        "    font-family: Arial, sans-serif;\n",
        "    margin: 0 0 10px 0; /* Reduce the bottom margin for more compact headers */\n",
        "    padding: 0; /* Remove padding */\n",
        "    line-height: 1.2; /* Control the line spacing */\n",
        "}\n",
        "\n",
        "/* Adjust table and image styles */\n",
        "table {\n",
        "    width: 100%; /* Make table full width within the 900px body */\n",
        "    border-collapse: collapse;\n",
        "    max-width: 100%;\n",
        "    margin-left: auto;  /* Center the table */\n",
        "    margin-right: auto; /* Center the table */\n",
        "    overflow-x: auto; /* Allow horizontal scrolling if the table is too wide */\n",
        "    display: block;\n",
        "}\n",
        "\n",
        "table, th, td {\n",
        "    border: 1px solid lightgray;\n",
        "    padding: 8px;\n",
        "    text-align: left;\n",
        "}\n",
        "\n",
        "th {\n",
        "    background-color: #f2f2f2;\n",
        "}\n",
        "\n",
        "/* Custom figure sizing */\n",
        ".figure {\n",
        "    width: 100%; /* Ensure figures take full width within the 900px body */\n",
        "    margin-left: auto;  /* Center figure */\n",
        "    margin-right: auto; /* Center figure */\n",
        "}\n",
        "\n",
        "img {\n",
        "    max-width: 100%;  /* Ensure images take full width within the 900px body */\n",
        "    height: auto;\n",
        "    display: block;\n",
        "    margin-left: auto;  /* Center image */\n",
        "    margin-right: auto; /* Center image */\n",
        "}\n",
        "</style>\n",
        "<!-- build with:  ./build.sh -p 4 -h -->\n",
        "\n",
        "<p style=\"text-align: center;\">\n",
        "  Project 4 Github: [<a href=\"https://github.com/tonythor/data622/project4.qmd\" target=\"_blank\">Quarto Presentation</a>] &nbsp; \n",
        "          [<a href=\"https://github.com/tonythor/data622/mnist.py\" target=\"_blank\">Python</a>] &nbsp; \n",
        "   &nbsp; | &nbsp; \n",
        "  Projects: [<a href=\"https://rpubs.com/tonythor/data622-project4\" target=\"_blank\" >4</a>] &nbsp; \n",
        "  [<a href=\"https://rpubs.com/tonythor/data622-project3\" target=\"_blank\" >3</a>] &nbsp; \n",
        "  [<a href=\"https://rpubs.com/tonythor/data622-project2\" target=\"_blank\" >2</a>] &nbsp; \n",
        "  [<a href=\"https://rpubs.com/tonythor/data622-project1\" target=\"_blank\" >1</a>] &nbsp; \n",
        "  \n",
        "</p>\n"
      ],
      "id": "695f8ccf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import display, HTML\n",
        "from lussi.mnist import *\n",
        "\n",
        "X_train, X_test, y_train, y_test, X_train_unscaled = load_data() "
      ],
      "id": "186a194d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Executive Summary\n",
        "\n",
        "This project evaluates the performance of K-Nearest Neighbors (KNN) and Neural Networks for handwritten digit recognition using the MNIST dataset. The analysis reveals several key findings:\n",
        "\n",
        "Performance:\n",
        "\n",
        "- The Neural Network achieved 97.3% accuracy, outperforming KNN's 94.7% accuracy\n",
        "- The Neural Network showed more consistent performance across all digits, with accuracy ranging from 95.6% to 98.6%\n",
        "- KNN showed more variability, with accuracy ranging from 89.9% to 99.2%\n",
        "\n",
        "Computational Characteristics:\n",
        "\n",
        "- Training: KNN trained in 6.38 seconds vs. Neural Network's 11.37 seconds\n",
        "- Prediction Speed: \n",
        "  - For small batches (1-100 images), KNN was faster\n",
        "  - For larger batches (1000 images), Neural Network was significantly faster (0.06ms vs 0.69ms per image)\n",
        "\n",
        "Error Patterns:\n",
        "\n",
        "- Both models struggled most with visually similar digits (e.g., 3/5, 4/9, 7/9)\n",
        "- KNN showed higher error rates for complex digits like '8' (89.9% accuracy)\n",
        "- Neural Network maintained >95% accuracy across all digit classes\n",
        "\n",
        "This analysis demonstrates that while KNN offers faster training and competitive performance for small-scale predictions, the Neural Network provides superior accuracy and better scaling for larger batch predictions, making it more suitable for production deployment despite longer training times.\n",
        "\n",
        "# Project Overview\n",
        "\n",
        "## History and Significance of MNIST\n",
        "\n",
        "The [MNIST dataset](https://yann.lecun.com/exdb/mnist/) (Modified National Institute of Standards and Technology) emerged from a practical need at the U.S. Postal Service in the late 1980s. It was created to help automate mail sorting by recognizing handwritten zip codes. Created by Yann LeCun, Corinna Cortes, and Christopher Burges, MNIST has become the de facto \"Hello World\" of machine learning. The dataset consists of 70,000 handwritten digits (60,000 for training, 10,000 for testing). Its standardized format and manageable size have made it an ideal benchmark for comparing machine learning algorithms for over three decades.\n",
        "\n",
        "### Understanding the Dataset Format \n",
        "\n",
        "Though easily converted, the records are not actually stored as images. They are stored as a matrix. Each record of the 60,000 images is stored as a 28 by 28 matrix, with those positions holding the color of the pixel the position represents. It's a square image, so 28 pixels by 28 pixels tall totals 784 total pixles, or 784 total numbers. Each of those numbers represents a shade of grayscale, 0 being all black, and 255 being white.\n"
      ],
      "id": "c40d3528"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#visualize_digit_matrix(X_train_unscaled, index=0)\n",
        "matrix_html = visualize_digit_matrix_encoded(X_train_unscaled, index=0)\n",
        "display(HTML(matrix_html))"
      ],
      "id": "73f9f0bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wrapping around each of those records, it's like any other machine learning dataset, test and train, and each of those two are broken apart into data and label, like so: \n",
        "\n",
        "**Training Set:**\n",
        "\n",
        "- Images: X_train →  60000  images, each of size  28 \\times 28 \n",
        "- Labels: y_train →  60000  labels, e.g., [5, 0, 4, 1, 9, ...]\n",
        "\n",
        "**Testing Set:**\n",
        "\n",
        "- Images: X_test →  10000  images, each of size  28 \\times 28 \n",
        "- Labels: y_test →  10000  labels, e.g., [7, 2, 1, 0, 4, ...]\n",
        "\n",
        "\n",
        "### Looking at Sample records\n",
        "\n",
        "It's easy to understand the core challenge by looking at records. There is much variation in hand written letters, with all sorts of factors presenting like: \n",
        "\n",
        "* Writing styles and penmanship\n",
        "* Stroke thickness and continuity\n",
        "* Digit orientation and slant\n",
        "* Image noise and quality\n"
      ],
      "id": "409d8641"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_html = plot_sample_digits_encoded(X_train_unscaled, y_train)\n",
        "display(HTML(samples_html))\n",
        "# plot_sample_digits(X_train_unscaled, y_train)"
      ],
      "id": "798188df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Goals\n",
        "\n",
        "This project aims to:\n",
        "\n",
        "* Compare the effectiveness of a simple, intuitive algorithm (KNN) against a more complex, modern approach (Neural Networks)\n",
        "* Analyze the tradeoffs between computational complexity and accuracy\n",
        "* Understand how different architectures handle the variations in handwritten digits\n",
        "* Evaluate both training time and inference speed for real-world applicability\n",
        "\n",
        "\n",
        "# Model Implementation and Training\n",
        "\n",
        "## KNN\n",
        "\n",
        "**K-Nearest Neighbors (KNN)** is a non-parametic algorithm, supervised machine algorithm used for both classification and regression tasks. The fundamental principle of KNN is simple: classify a new data point based on the majority vote (classification) or average (regression) of its K nearest neighbors in the feature space. \n"
      ],
      "id": "4f3a9e2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train KNN model\n",
        "print(\"Training KNN Model...\")\n",
        "start_time = time.time()\n",
        "knn_model, knn_accuracy = train_knn(X_train, X_test, y_train, y_test, rebuild_model=False)\n",
        "knn_train_time = time.time() - start_time"
      ],
      "id": "49180f03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Configuration\n",
        "\n",
        "- **Data Splitting:** \n",
        "    - Used *train_test_split()* function to split the data:\n",
        "        - Training (80%)\n",
        "        - Testing (20%)\n",
        "    - An 80-20 split balances training data sufficiency and evaluation robustness\n",
        "- **Feature Scaling:**\n",
        "    - Used *StandardScalar()* to scale the data, ensuring all features contribute equally to the distance calculation.\n",
        "- **Lazy Learning:**\n",
        "    - KNN stores the entire training dataset and only makes predictions during inference\n",
        "\n",
        "\n",
        "\n",
        "**Prediction Process Mechanics:**\n",
        "\n",
        "1. **Distance Calculation**\n",
        "    - For the new data point, calculate its distance to all training points. \n",
        "    - The default distance calculation method is Euclidean distance.\n",
        "    - Euclidean Distance formula: $d(p, q) = \\sqrt{\\sum_{i=1}^n (q_i - p_i)^2}$\n",
        "2. **Neighbor Selection** \n",
        "    - Select the K closest points (neighbors)\n",
        "    - In this implementation, n_neighbors =3, making the prediction sensitive to local patterns.\n",
        "3. **Classification Method**\n",
        "    - Majority voting determines the class\n",
        "    - Most frequent class among K neighbors wins\n",
        "\n",
        "\n",
        "\n",
        "**Key Parameters:**\n",
        "\n",
        "- **Number of Neighbors: n_neighbors = 3**\n",
        "    * A small value (e.g., 3) captures local patterns but is prone to overfitting.\n",
        "    * Larger values smoothen predictions but may underfit.\n",
        "- **Distance Metric:**\n",
        "    * Default is Euclidean distance\n",
        "    * Other options include Manhattan or Minkowski distances for varying use cases.\n",
        "- **Weighting Scheme:**\n",
        "    * Default is Uniform, where all neighbors contribute equally.\n",
        "    * Weighted options give more influence to closer neighbors.\n",
        "\n",
        "\n",
        "\n",
        "### Performance Metrics"
      ],
      "id": "a7f32178"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"\\nKNN Results:\")\n",
        "print(f\"Training Time: {knn_train_time:.2f} seconds\")\n",
        "print(f\"Accuracy: {knn_accuracy:.4f}\")"
      ],
      "id": "e47a157b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "K-Nearest Neighbors offers a straightforward yet powerful approach to classification. By leveraging local neighborhood information and flexible distance calculations, KNN provides an interpretable method for pattern recognition in machine learning tasks.\n",
        "\n",
        "\n",
        "\n",
        "## Neural Network\n",
        "\n",
        "A **neural network** is a machine learning algorithm inspired by the structure and function of the human brain. It is designed to learn relationships, recognize patterns, and make predictions by mimicking how biological neurons process and transmit information. Neural networks excel in handling complex, non-linear data, making them a versatile tool for tasks such as image recognition, natural language processing, and classification.\n"
      ],
      "id": "8090c1e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nTraining Neural Network...\")\n",
        "start_time = time.time()\n",
        "nn_model, history, nn_accuracy = train_neural_network(X_train, X_test, y_train, y_test, rebuild_model=False)\n",
        "nn_train_time = time.time() - start_time"
      ],
      "id": "96d600e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture Overview\n",
        "\n",
        "We created a neural network designed for MNIST digit classification. It features a multi-layer feedforward architecture with strategic layer design and regularization techniques.\n",
        "\n",
        "\n",
        "**Detailed Layer Analysis:**\n",
        "\n",
        "1. Input Layer\n",
        "    - **Dimensions:** 784 neurons (28 x 28 pixel flattened image)\n",
        "    - **Purpose:** Direct mapping of pixel intensity values\n",
        "    - **Transformation:** Converts 2D image to 1D feature vector\n",
        "\n",
        "2. First Hidden Layer\n",
        "    - **Dimensions:** 256 neurons\n",
        "    - **Activation:** ReLU (Rectified Linear Unit)\n",
        "    - **Objectives:**\n",
        "        - Initial complex feature extraction\n",
        "        - Introduces non-linear transformations\n",
        "        - Captures primary image characteristics\n",
        "\n",
        "3. First Dropout layer\n",
        "    - **Dropout Rate:** 0.2 (20%)\n",
        "    - **Regularization Technique:**\n",
        "        - Randomly deactivates 20% of neurons during training\n",
        "        - Prevents model overfitting\n",
        "        - Reduces neuron interdependence\n",
        "\n",
        "4. Second Hidden Layer\n",
        "    - **Dimensions:** 128 neurons\n",
        "    - **Activation:** ReLU (Rectified Linear Unit)\n",
        "    - **Objectives:**\n",
        "        - Further abstract feature representations\n",
        "        - Progressively reduce feature dimensionality\n",
        "        - Refine initial feature extraction\n",
        "\n",
        "5. Second Dropout Layer\n",
        "    - **Dropout Rate:** 0.2 (20%)\n",
        "    - **Continues regularization strategy**\n",
        "    - Prevents neural network from becoming too specialized\n",
        "\n",
        "6. Third Hidden Layer\n",
        "    - **Dimensions:** 64 neurons\n",
        "    - **Activation:** ReLU (Rectified Linear Unit)\n",
        "    - **Objectives:**\n",
        "        - Final feature abstraction\n",
        "        - Prepares data for classification\n",
        "        - Further reduces feature complexity\n",
        "\n",
        "7. Output Layer\n",
        "    - **Neurons:** 10 (one per digit 0-9)\n",
        "    - **Activation:** Softmax\n",
        "    - **Characteristics:**\n",
        "        - Converts raw scores to probability distribution\n",
        "        - Ensures probabilities sum to 1\n",
        "        - Enables multi-class classification\n",
        "    \n",
        "\n",
        "\n",
        "### Training Configuration\n",
        "\n",
        "- **Hyperparameters:**\n",
        "\n",
        "    1. epochs\n",
        "        - **Total Iterations:** 10\n",
        "        - **Purpose:**\n",
        "            - Complete passes through entire training dataset\n",
        "            - Allows progressive weight refinement\n",
        "            - Prevents overfitting through limited iterations\n",
        "\n",
        "    2. batch_size\n",
        "        - **Configuration:** 128 samples per gradient update\n",
        "        - **Benefits:**\n",
        "            - Computational efficiency\n",
        "            - Gradient noise reduction\n",
        "            - Memory-friendly processing\n",
        "\n",
        "    3. validation_split\n",
        "        - **Allocation:** 10% of the training data\n",
        "        - **Functions:**\n",
        "            - Monitor model performance during training\n",
        "            - Detect potential overfitting \n",
        "            - Provide real-time performance insights\n",
        "\n",
        "- **Optimization Strategy:**\n",
        "    - Adam\n",
        "        - Adaptive learning rate optimization\n",
        "        - Characteristics:\n",
        "            - Combines RMSprop and momentum advantages\n",
        "            - Dynamically adjusts per-parameter learning rates\n",
        "            - Handles sparse gradients effectively\n",
        "\n",
        "\n",
        "- **Loss Function:**\n",
        "    - Sparse Categorical Cross-Entropy\n",
        "        - Ideal for multi-class classification\n",
        "        - Measures:\n",
        "            - Difference between predicted and actual distributions\n",
        "            - Guides weight updates during backpropagation\n",
        "\n",
        "\n",
        "\n",
        "### Performance Metrics"
      ],
      "id": "f828c58e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"\\nNeural Network Results:\")\n",
        "print(f\"Training Time: {nn_train_time:.2f} seconds\")\n",
        "print(f\"Accuracy: {nn_accuracy:.4f}\")"
      ],
      "id": "20c62844",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion \n",
        "\n",
        "The neural network architecture is carefully designed to balance complexity, feature extraction, and generalization. By incorporating strategic layer design, dropout regularization, and adaptive optimization, the model achieves robust performance in MNIST digit classification.\n",
        "\n",
        "\n",
        "# Model Comparison\n",
        "\n",
        "In this section, we compare the performance of the K-Nearest Neighbors (KNN) algorithm and the Neural Network (NN) architecture based on key performance metrics: training time and accuracy.\n",
        "\n",
        "## Performance Metrics\n"
      ],
      "id": "cabecdac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "compare_df = create_comparison_table(knn_accuracy, knn_train_time, nn_accuracy, nn_train_time)\n",
        "print(compare_df)"
      ],
      "id": "8407c6df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Training Time**\n",
        "    - KNN exhibits a faster training process 6.38 seconds) since it is a \"lazy learning\" algorithm, which delays most computation until prediction.\n",
        "    - The Neural Network, being a \"eager learning\" algorithm, spends more time (13.84 seconds) in training due to backpropagation, weight updates, and regularization techniques.\n",
        "\n",
        "2. **Accuracy:**\n",
        "    - The Neural Network outperforms KNN with an accuracy of **97.3%**, compared to **94.7%** for KNN.\n",
        "    - The Neural Network's higher accuracy is attributed to its ability to extract complex, non-linear patterns in the data through multiple layers and activation functions.\n",
        "    - KNN, while simpler, relies on proximity in the feature space, which may not fully capture intricate relationships.\n",
        "\n",
        "3. **Scalability:**\n",
        "    - KNN's computational cost increases significantly with larger datasets or higher-dimensional data due to the need to calculate distances for all training samples during prediction.\n",
        "    - Neural Networks scale better for larger datasets, as training is done once, and predictions are efficient after model training.\n",
        "\n",
        "## Model Accuracy by Digit\n"
      ],
      "id": "6c7932f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_text, knn_cm_percent, nn_cm_percent = analyze_model_accuracies(knn_model, nn_model, X_test, y_test)\n",
        "\n",
        "# Then create and display the visualization\n",
        "comparison_viz = compare_model_accuracies_encoded(knn_model, nn_model, X_test, y_test)\n",
        "\n",
        "# Display both\n",
        "# print(analysis_text)\n",
        "display(HTML(comparison_viz))"
      ],
      "id": "e6bd04e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-Class Performance Analysis\n",
        "Let's analyze how each model performs for different digits:"
      ],
      "id": "1f439db3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nDetailed Per-Digit Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "for i in range(10):\n",
        "    knn_accuracy_per_class = knn_cm_percent[i,i]\n",
        "    nn_accuracy_per_class = nn_cm_percent[i,i]\n",
        "    \n",
        "    print(f\"\\nDigit {i}:\")\n",
        "    print(f\"KNN Accuracy: {knn_accuracy_per_class:.1f}%\")\n",
        "    print(f\"Neural Network Accuracy: {nn_accuracy_per_class:.1f}%\")\n",
        "    print(f\"Difference: {(nn_accuracy_per_class - knn_accuracy_per_class):.1f}%\")"
      ],
      "id": "f0906b3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Speed Analysis\n",
        "To understand real-world performance implications, let's analyze prediction speeds for different batch sizes:"
      ],
      "id": "d19a459e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "batch_sizes = [1, 10, 100, 1000]\n",
        "results = {'knn': {}, 'nn': {}}\n",
        "print(\"\\nPrediction Speed Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "for batch_size in batch_sizes:\n",
        "    # Select subset of test data\n",
        "    X_batch = X_test[:batch_size]\n",
        "    \n",
        "    # KNN timing\n",
        "    start_time = time.time()\n",
        "    _ = knn_model.predict(X_batch)\n",
        "    knn_time = time.time() - start_time\n",
        "    results['knn'][batch_size] = knn_time\n",
        "    \n",
        "    # Neural Network timing\n",
        "    start_time = time.time()\n",
        "    _ = nn_model.predict(X_batch, verbose=0)\n",
        "    nn_time = time.time() - start_time\n",
        "    results['nn'][batch_size] = nn_time\n",
        "    \n",
        "    print(f\"\\nBatch size: {batch_size}\")\n",
        "    print(f\"KNN prediction time: {knn_time:.4f} seconds\")\n",
        "    print(f\"Neural Network prediction time: {nn_time:.4f} seconds\")\n",
        "    print(f\"Time per image - KNN: {(knn_time/batch_size)*1000:.2f}ms\")\n",
        "    print(f\"Time per image - NN: {(nn_time/batch_size)*1000:.2f}ms\")"
      ],
      "id": "f8daa425",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Key Findings and Business Impact\n",
        "\n",
        "## Overall Accuracy"
      ],
      "id": "99fb591b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "print(f\"Neural Network: {nn_accuracy:.4f} (97.3%)\")\n",
        "print(f\"KNN: {knn_accuracy:.4f} (94.7%)\")\n",
        "print(f\"The Neural Network provides a {97.3 - 94.7:.1f} percentage point higher accuracy.\")"
      ],
      "id": "f2506351",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Performance"
      ],
      "id": "5812af7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "print(f\"KNN Training Time: {knn_train_time:.2f} seconds\")\n",
        "print(f\"Neural Network Training Time: {nn_train_time:.2f} seconds\")"
      ],
      "id": "0d8cc4e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Speed\n",
        "\n",
        "Small batches (1-100 images): KNN performs faster\n",
        "Large batches (1000+ images): Neural Network shows superior performance\n",
        "Neural Network scales better for production workloads\n",
        "\n",
        "\n",
        "## Error Analysis\n",
        "\n",
        "Both models struggle most with visually similar digits (3/5, 4/9, 7/9)\n",
        "Neural Network shows more consistent performance across all digit classes\n",
        "KNN shows higher variability in accuracy between different digits\n",
        "\n",
        "\n",
        "## Business Implications\n",
        "\n",
        "For real-time, single-image processing: KNN might be preferable due to faster prediction times\n",
        "For batch processing: Neural Network is clearly superior\n",
        "Trade-off between setup time (KNN faster) vs long-term performance (NN better)\n",
        "Memory requirements favor Neural Network for large-scale deployment\n",
        "\n",
        "\n",
        "## Deployment Considerations\n",
        "\n",
        "KNN requires storing entire training dataset (higher memory usage)\n",
        "Neural Network has fixed memory footprint after training\n",
        "Neural Network offers better scalability for production systems\n",
        "\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "- Neural Network (NN) clearly outperforms the K-Nearest Neighbors (KNN) model in terms of both accuracy and handling more complex patterns in the data. It also shows better scalability as the dataset grows.\n",
        "- KNN is still a useful algorithm for simpler datasets or when interpretability and speed are more important than accuracy, but Neural Networks are better suited for high-accuracy tasks, especially with larger and more complex datasets.\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **KNN** is advantageous for smaller datasets and when simplicity and interpretability are priorities.\n",
        "- **Neural Networks** are ideal for larger or more complex datasets where advanced feature extraction and higher accuracy are desired."
      ],
      "id": "8562f372"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/kristinlussi/Documents/GitHub/data622/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}