---
title: "Project 1: Pay and our industry"
author: "Team: I Love Lucy"
date: "10 Oct 2024"
output:
  html_document:
    toc: true
    number_sections: true
    self_contained: true
python: 
  jupyter: data622
execute:
#  echo: false
  warning: false
  message: false
  freeze: auto
---

<style>
/* Quarto Meta Title Styling */
.quarto-title-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    flex-wrap: wrap;
}

.quarto-title-meta-heading {
    font-weight: bold;
}

.quarto-title-meta-contents {
    margin-right: 20px;
}

/* General Layout Styling */
body {
    width: 900px;
    font-family: Arial, sans-serif;
    margin: 0 auto;
    background-color: white;
}

/* Header Container Styling */
.header-container {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 20px;
}

.header-container h1 {
    margin: 0;
    font-size: 2.5em;
}

.header-container .meta-info {
    text-align: right;
    font-size: 1.2em;
    margin: 0;
}

/* Header Font Styling */
h1, h2, h3, h4, h5, h6 {
    font-family: Arial, sans-serif;
    margin: 0 0 10px 0;
    padding: 0;
    line-height: 1.2;
}

/* Table Styling */
table {
    width: 100%;
    border-collapse: collapse;
    margin: 0 auto;
    table-layout: fixed; /* Ensures even column widths */
}

table.dataTable {
    width: 100% !important;
    table-layout: fixed;
}

/* Header Styling for DataTable */
thead th {
    padding: 10px;
    background-color: #f2f2f2;
    text-align: left;
    position: sticky;
    top: 0;
    z-index: 100;
}

/* Body Cell Styling */
table.dataTable td,
table.dataTable th {
    padding: 8px;
    border: 1px solid lightgray;
    vertical-align: middle;
}

/* DataTables Wrapper Styling */
div.dataTables_wrapper {
    width: 100%;
    margin: 0 auto;
}

/* Scrolling Controls */
.dataTables_scrollBody {
    max-height: 300px;
    overflow-y: auto;
    overflow-x: hidden;
    width: 100%; 
}

.dataTables_scrollHead {
    overflow: hidden;
}

.dataTables_scrollHeadInner {
    width: 100% !important;
}

/* Prevent Independent Scrolling Issues */
.dt-scroll-body, 
.dt-scroll-head, 
.dt-container, 
.dt-layout-table {
    border: none !important;
    margin: 0 !important;
    padding: 0 !important;
}

/* Image Styling */
img {
    max-width: 100%;
    height: auto;
    display: block;
    margin-left: auto;
    margin-right: auto;
}

/* Figure Styling */
.figure {
    width: 100%;
    margin-left: auto;
    margin-right: auto;
}
</style>
<p style="text-align: center;">
  [github](https://github.com/tonythor/data622) &nbsp; | &nbsp; [web presentation](https://rpubs.com/tonythor/data622-project1)
</p>




# Data Sets

This project analyzes two datasets: one from ZipRecruiter and the other from Stack Overflow.

•	ZipRecruiter Dataset: A heavily curated, clean, and well-structured dataset. It offers a tight focus, making it easy to work with and delivering highly accurate predictions.

•	Stack Overflow Dataset: A generation of self-reported survey data, providing valuable insights but requiring more effort to clean and analyze. As we’ll see, while the ZipRecruiter dataset enables near-perfect predictions, the Stack Overflow data presents more challenges due to variability and inconsistencies.

## Load
```{python import_and_load, message=false, warning=false, echo=false}
import io
import base64
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
import pandas as pd
import matplotlib.ticker as mticker
from mpl_toolkits.axes_grid1 import make_axes_locatable
from itables import show
from lussi.ziprecruiter import *
from lussi.stackoverflow import * 
data_dir = "622data_nogit"
ziprecruiter = load_zip(data_dir = data_dir)
wide_stack = load_stack(data_dir=data_dir, stack_type=StackType.WIDE)
```

## Data Set 1: Zip Recruiter 
First, let’s view a sample of the Zip Recruiter dataset:
```{python ziprecruiter}
ziprecruiter.sample(n=10, random_state=42).head(10)
```
```{python plot1, message=false, warning=false, echo=false, fig.width=8, fig.height=5}
# Note that becuase we are publishing to the web, we will be base64 encoding our 
#images directly into this web page.
df = ziprecruiter # so we don't overwrite anything! 
plt.figure(figsize=(8, 5))
sns.boxplot(y='Job Title', x='Annual Salary', data=df, orient='h')
plt.title('Salary by title within this data set')
plt.ylabel('')  # Remove the label on the y-axis
img_buf = io.BytesIO()
plt.savefig(img_buf, format='png', bbox_inches='tight')  # Save figure to buffer
plt.close()  # Prevents Quarto from auto-rendering the plot. 
img_buf.seek(0) ## reset the buffer
img_base64 = base64.b64encode(img_buf.read()).decode('utf-8')
img_html = f'<img src="data:image/png;base64,{img_base64}" alt="Salary by title" />'
# And render. There is no cached image!
from IPython.display import display, HTML
display(HTML(img_html))
```

### Overview of the Dataset

This dataset was scraped from ZipRecruiter using Selenium and Chromedriver. We collected several pages containing salary information by job title and state, such as a page for [data engineers](https://www.ziprecruiter.com/Salaries/What-Is-the-Average-DATA-Engineer-Salary-by-State){target="_blank"}. Additionally, we added state abbreviations and a salary tier column to provide more granular analysis.

Though relatively small, the dataset is clean, standardized, and well-suited for both analysis and prediction. Its structure makes it an excellent candidate for regression-based algorithms such as Linear Regression or Random Forest, allowing us to predict salaries based on key features like job title and state.

### Model Evaluation

```{python lr}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# One-hot encode categorical state and job title
df = pd.get_dummies(ziprecruiter, columns=['State', 'Job Title'], drop_first=True)

# Split the data into features (X) and target (y)
X = df.drop(['Annual Salary', 'Abbreviation', 'Salary Tier'], axis=1)
y = df['Annual Salary']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

y_pred = lr_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print(f'Mean Squared Error: {mse:.2f}')
print(f'R² Score: {r2:.5f}')

```

The linear regression model performed exceptionally well:

•	Mean squared error : 10.70. This low MSE indicates that the model’s 
    predictions are highly accurate and closely align with the actual values.

•	R² score: 0.99999997. An R² score near 1 suggests that the model 
    explains nearly all the variability in annual salaries. 

### Conclusion
The ZipRecruiter dataset shines due to its clean structure and high-quality data. With straightforward, well-labeled columns, it is a dream for regression models. The linear regression model performed exceptionally well here, with a near-perfect R² score and minimal error. This demonstrates that linear relationships between job title, location, and salary are sufficient to predict outcomes accurately.

From a business perspective, this dataset provides actionable insights. Recruiters and job seekers can rely on these predictions to benchmark salaries based on roles and geography. However, its curated nature may limit the depth of insight—since the data is pre-filtered for clear patterns, it leaves less room for exploratory or advanced analysis beyond standard predictions.


## Stack Overflow
Now let's look at the wide Stack Overflow dataset, and be sure scroll to the right.: 
```{python ziprecruiter}

show(wide_stack.sample(n=10, random_state=42), scrollY='300px', paging=True, scrollX=True)
# wide_stack.sample(n=10, random_state=42).head(10)
```
This dataset originates from the [Stack Overflow user-entered survey data](https://survey.stackoverflow.co), contains about half a million records and spans between 2017 through 2023. We uploaded the raw survey data to an S3 bucket, and using the code in the lussi python namespace of this project, we standardized and extracted many columns with the hope of predicting annual salary. 


### Stack Overflow columns used percentage, by year
Out of gate, a big chunk of this data is not usable. Have a look at this chart, it shows some columns that just aren't included between years. 

```{python plot2, message=false, warning=false, echo=false, fig.width=8, fig.height=9}
grouped_summary_filtered = wide_stack.groupby("Year").agg(lambda x: (x.notnull().mean() * 100)).reset_index()
# Round the result to one decimal place
grouped_summary_filtered_rounded = grouped_summary_filtered.round(1)
# Display the grouped summary
# from IPython.display import display
# display(grouped_summary_filtered_rounded)
show(
    grouped_summary_filtered_rounded, 
    scrollY='300px', 
    scrollX=True, 
    paging=True, 
    datatables_options={
        'scrollCollapse': True,
        'fixedHeader': True
    }
)
# show(grouped_summary_filtered_rounded, scrollY='300px', paging=True, scrollX=True)

```

### Selecting a model

Extreme Gradient Boosting (XGBoost) is our first and best choice. Our data is all over the map. XGBoost is going to be robust to missing values, scalable, friendly to nonlinear relationships, good at handling outliers, and good for generalization of trends across years.

Models like Linear Regression (LR) aren't a good fit. LR assumes that feature relationships are purely linear. It also doesn't like outliers.  

We'll run use Random Forest as a control, but it won't be good either. There is risk of over-fitting, we'd have to impute data, it'll be slow, and doesn't  like highly dimensional features. 

### Further preparing
```{python furtherEngineering, message=false, warning=false, echo=false, fig.width=8, fig.height=9}

def group_gender(gender):
    gender = str(gender).lower()  # Converts to lowercase
    if 'female' in gender or 'woman' in gender:
        return 'Female'
    elif 'male' in gender or 'man' in gender:
        return 'Male'
    else:
        return 'Other'
        
def parse_devtype(df):
    devtype_dummies = df['DevType'].str.get_dummies(sep=';')  # One-hot encode DevType roles
    return pd.concat([df, devtype_dummies], axis=1)  # Add the encoded roles to the DataFrame
    
# Step 2: Apply transformations, including parsing and encoding 'DevType'
filtered_stack = (
    wide_stack
    .query("Year != 2019 and Year != 2020")  # No salary
    .query("Year != 2017")  # No age
    .query("Country == 'United States'")  # Forget that!
    .assign(gender_grouped=wide_stack['Gender'].apply(group_gender))  # Add gender_grouped, simplify to m/f/o
    .pipe(parse_devtype)  # Add in DevType columns
    .drop('Gender', axis=1) 
    .drop('Sexuality', axis=1)  # DE: sexuality_grouped as either straight or lgbtq+
    .drop('Ethnicity', axis=1)  # DE: ethnicity_grouped, either minority or non-minority
    .drop('PlatformWorkedWith', axis=1)  # DE: expanded in calculated columns
    .drop('LanguageWorkedWith', axis=1)  # DE: expanded in calculated columns
    .drop('DatabaseWorkedWith', axis=1)  # DE: expanded in calculated columns
    .drop('US_State', axis=1)  # Too few
    .drop('Country', axis=1) 
    .drop('YearsCodePro', axis=1)  # DE: Replaced by YearsCodeProAvg
    .drop('OrgSize', axis=1)  # DE: Handled by OrgSizeAvg
    .drop('Age', axis=1)  # DE: Replaced with average age
    .drop('DevType', axis=1)  # Already expanded into one-hot columns
    .query('AnnualSalary > 1')   
)

print(f"Length of filtered_stack is: {len(filtered_stack)}") 
# filtered_columns = wide_stack
# Group and calculate the percentage of non-null values for the filtered columns
grouped_summary_filtered = filtered_stack.groupby("Year").agg(lambda x: (x.notnull().mean() * 100)).reset_index()
# Round the result to one decimal place
grouped_summary_filtered_rounded = grouped_summary_filtered.round(1)
# Display the grouped summary
show(grouped_summary_filtered_rounded, scrollY='300px', paging=True, scrollX=True)
show(filtered_stack.sample(n=10, random_state=42), scrollY='300px', paging=True, scrollX=True)
```

### Random Forest First
In trying this, our initial performane was pretty terrible. We've already adjusted this by adding in kfold validation. 

```{python randomForest, message=false, warning=false, echo=false, fig.width=8, fig.height=9}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import io
import base64
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from IPython.display import display, HTML

# Step 1: Identify categorical columns and fill missing values if necessary
df = filtered_stack.fillna('Unknown')  # Handle missing values

# Step 2: Separate the target and features before encoding
y = df['AnnualSalary']  # Target variable
X = df.drop(['AnnualSalary', 'Year'], axis=1)  # Drop target and irrelevant columns

# Step 3: Apply one-hot encoding to features only
X_encoded = pd.get_dummies(X, drop_first=True)

# Step 4: K-Fold Cross-Validation Setup
kfold = KFold(n_splits=5, shuffle=True, random_state=42)  # 5 folds

# Step 5: Train the RandomForestRegressor with Cross-Validation
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Step 6: Perform Cross-Validation
cv_scores = cross_val_score(rf_model, X_encoded, y, cv=kfold, scoring='neg_mean_squared_error')

# Convert negative MSE to positive for interpretation
mse_scores = -cv_scores
rmse_scores = np.sqrt(mse_scores)

print(f"Cross-Validation MSE Scores: {mse_scores}")
print(f"Cross-Validation RMSE Scores: {rmse_scores}")
print(f"Average RMSE: {rmse_scores.mean():.2f}")

# Step 7: Train on Full Training Data and Evaluate
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)
rf_model.fit(X_train, y_train)

# Step 8: Make Predictions and Evaluate the Model
y_pred = rf_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error
mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error
rmse = np.sqrt(mse)  # Root Mean Squared Error
r2 = r2_score(y_test, y_pred)  # R² Score

# Print Evaluation Metrics
print(f"Test Set Metrics:")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R² Score: {r2:.5f}")

# Step 9: Get feature importances from the trained model
importances = rf_model.feature_importances_

# Step 10: Get the feature names from the encoded DataFrame
feature_names = X_encoded.columns

# Step 11: Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Step 12: Select the top 15 most important features (optional)
top_n = 15
top_indices = indices[:top_n]

# Step 13: Create the plot and save it to a buffer
plt.figure(figsize=(10, 6))
plt.barh(
    [feature_names[i] for i in top_indices], 
    importances[top_indices], 
    align='center'
)
plt.xlabel('Feature Importance')
plt.title('Top Feature Importance Random Forest Regressor')
plt.gca().invert_yaxis()  # Invert y-axis to display the most important feature on top

# Save the plot to a BytesIO buffer
img_buf = io.BytesIO()
plt.savefig(img_buf, format='png', bbox_inches='tight')
plt.close()  # Close the plot to free up memory

# Reset the buffer position to the beginning
img_buf.seek(0)

# Convert the buffer content to a base64 string
img_base64 = base64.b64encode(img_buf.read()).decode('utf-8')

# Create an HTML <img> tag with the base64-encoded image
img_html = f'<img src="data:image/png;base64,{img_base64}" alt="Random Forest Feature Importance" />'

# Display the HTML with the embedded image
display(HTML(img_html))

```

### XGBoost

```{python randomForest, message=false, warning=false, echo=false, fig.width=8, fig.height=9}

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
import io
import base64
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from IPython.display import display, HTML

# Step 1: Prepare the data
df = filtered_stack.fillna('Unknown')  # Handle missing values
y = df['AnnualSalary']  # Target variable
X = df.drop(['AnnualSalary', 'Year'], axis=1)  # Drop target and irrelevant columns

# Step 2: Apply one-hot encoding
X_encoded = pd.get_dummies(X, drop_first=True)

# Step 3: Set up K-Fold Cross-Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Step 4: Train the XGBoost Regressor with Cross-Validation
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)

# Step 5: Perform Cross-Validation
cv_scores = cross_val_score(xgb_model, X_encoded, y, cv=kfold, scoring='neg_mean_squared_error')

# Convert negative MSE to positive for interpretation
mse_scores = -cv_scores
rmse_scores = np.sqrt(mse_scores)

print(f"Cross-Validation MSE Scores: {mse_scores}")
print(f"Cross-Validation RMSE Scores: {rmse_scores}")
print(f"Average RMSE: {rmse_scores.mean():.2f}")

# Step 6: Train on Full Training Data and Evaluate
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)
xgb_model.fit(X_train, y_train)

# Step 7: Make Predictions on the Test Set
y_pred = xgb_model.predict(X_test)

# Step 8: Evaluate the Model
mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error
mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error
rmse = np.sqrt(mse)  # Root Mean Squared Error
r2 = r2_score(y_test, y_pred)  # R² Score

# Print Test Set Metrics
print(f"\nTest Set Metrics:")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R² Score: {r2:.5f}")

# Step 9: Get feature importances from the XGBoost model
importances = xgb_model.feature_importances_

# Step 10: Get the feature names from the encoded DataFrame
feature_names = X_encoded.columns

# Step 11: Sort feature importances in descending order
indices = np.argsort(importances)[::-1]  # Sort in descending order

# Step 12: Select the top 15 most important features (optional)
top_n = 15
top_indices = indices[:top_n]

# Step 13: Create the plot and save it to a buffer
plt.figure(figsize=(10, 6))
plt.barh(
    [feature_names[i] for i in top_indices], 
    importances[top_indices], 
    align='center'
)
plt.xlabel('Feature Importance')
plt.title('Top Feature Importance XGBoost Regressor')
plt.gca().invert_yaxis()  # Invert y-axis to display the most important feature on top

# Save the plot to a BytesIO buffer
img_buf = io.BytesIO()
plt.savefig(img_buf, format='png', bbox_inches='tight')
plt.close()  # Close the plot to free up memory

# Reset the buffer position to the beginning
img_buf.seek(0)

# Convert the buffer content to a base64 string
img_base64 = base64.b64encode(img_buf.read()).decode('utf-8')

# Create an HTML <img> tag with the base64-encoded image
img_html = f'<img src="data:image/png;base64,{img_base64}" alt="XGBoost Feature Importance" />'

# Display the HTML with the embedded image
display(HTML(img_html))
```
## Conclusions, Stack Overflow Data

The Stack Overflow dataset provides rich self-reported information, capturing the diversity of real-world roles, company sizes, and technologies. However, this variety makes it difficult to create accurate predictions. Missing values and inconsistencies across survey years add further challenges. Critical factors like location or individual job performance, which are important for predicting salaries, are absent. As a result, it’s tough to generate reliable models from this data.

Even though it’s not ideal for predictive tasks, the dataset still offers value for exploring trends in the industry. It’s useful for understanding how salaries differ across technologies or tracking workplace diversity over time. While the data has gaps, it remains a solid tool for reporting and analysis.

## Model Performance and Practical Use

XGBoost and Random Forest fell short of expectations. Predictive accuracy of at least 75% is necessary for practical use, but the results here were far from that. Both models produced high error rates, with RMSE values over 800,000, and their negative R² scores show they struggled to find meaningful patterns. XGBoost, although suited for messy and complex datasets, didn’t perform well because the absence of key context limited what it could learn. Random Forest also struggled with the high-dimensional data and noise, requiring more preprocessing, which made it less practical. The combination of these factors suggests that predictions from this dataset are only slightly better than random guesses.

## Takeaways and Final Thoughts

While the Stack Overflow dataset isn’t suited for salary predictions, it still provides valuable insights. It can be used to explore trends, such as salary benchmarks for different technologies or the demographics of various job roles. For predictive modeling, however, curated datasets like ZipRecruiter or commercially available data with more context would be more reliable.

Moving forward, the focus should shift away from predictions and toward extracting insights from the data we have. If prediction remains a goal, combining datasets or finding one with detailed regional or performance metrics could improve results. For now, the strength of this dataset lies in the stories it can tell about the tech industry rather than in predicting individual salaries.


